<!doctype html>
<!--
Copyright 2016 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">

    <title>FOVEATION DYNRES</title>
    <style>
      #webgl-canvas, #presenting-message {
        box-sizing: border-box;
        height: 100%;
        left: 0;
        margin: 0;
        position: absolute;
        top: 0;
        width: 100%;
      }
      #presenting-message {
        color: white;
        font-family: sans-serif;
        font-size: 2em;
        font-weight: bold;
        z-index: 1;
        text-align: center;
        padding: 0.5em;
        background-color: #444;
        display: none;
      }
    </style>
    <script src="js/third-party/gl-matrix.js"></script>

    <script src="js/third-party/wglu/wglu-program.js"></script>
    <script src="js/third-party/wglu/wglu-stats.js"></script>
    <script src="js/third-party/wglu/wglu-texture.js"></script>

    <script src="js/vr-cube-sea.js"></script>
    <script src="js/vr-samples-util.js"></script>
  </head>
  <body>
    <canvas id="webgl-canvas"></canvas>
    <script>
      (function () {
      "use strict";

      var vrDisplay = null;
      const NUM_INSTANCES = 1;
      const NUM_ROTATIONS = 16;
      var multiview = 0;
      var frameData = null;

      var vrPresentButton = null;
      var is_multiview = false;
      var multiviewSupported = false;

      var webglCanvas = document.getElementById("webgl-canvas");
      var glAttribs = {
        alpha: true,
        depth: true,
        stencil: false,
        preserveDrawingBuffer : false,
        antialias : true,
        ovr_samples : 4
      };
      var gl = null;
      gl = webglCanvas.getContext("webgl2", glAttribs);
      if (!gl) {
        VRSamplesUtil.addError("Can't crate WebGL2 context");
        return;
      }
//      var ext = gl.getExtension('WEBGL_multiview');
    /*  if (ext) {
        console.log("MULTIVIEW extension is supported");
        multiviewSupported = true;
      }
      else
        console.log("MULTIVIEW extension is NOT supported");
     */
//      webglCanvas.width = 1024;
//      webglCanvas.height = 1024;

      gl.clearColor(0.1, 0.2, 0.3, 1.0);
      gl.enable(gl.DEPTH_TEST);
      gl.enable(gl.CULL_FACE);

      var vertexShaderSource = 
      "#version 300 es\n"+
//      "uniform vec2 u_offset;\n"+
//      "uniform vec2 u_scale;\n"+
//      "out mediump vec2 v_texcoord;\n"+
      "\n"+
      "void main()\n"+
      "{\n"+
      //  xy - coords of the quad, normalized to 0..1
      //  xy  - UV of the source texture coordinate.
      "    const vec2 quad_positions[6] = vec2[6]\n"+
      "    (\n"+
      "        vec2(0.0, 0.0),\n"+
      "        vec2(1.0, 0.0),\n"+
      "        vec2(0.0, 1.0),\n"+

      "        vec2(0.0, 1.0),\n"+
      "        vec2(1.0, 0.0),\n"+
      "        vec2(1.0, 1.0)\n"+
      "    );\n"+
      "\n"+
      "    gl_Position = vec4((quad_positions[gl_VertexID].xy * 2.0) - 1.0, 0.0, 1.0);\n"+
 //    "    gl_Position = vec4((quad_positions[gl_VertexID].xy), 0.0, 1.0);\n"+
//      "    v_texcoord = quad_positions[gl_VertexID].xy;\n"+
      "}\n";
/*
"#version 300 es\n";
      if (!multiviewSupported) {
        vertexShaderSource += "#define DISABLE_MULTIVIEW 1\n";
      }
      vertexShaderSource +=
      `#ifndef DISABLE_MULTIVIEW
        #define DISABLE_MULTIVIEW 0
       #endif
      #define NUM_VIEWS 2
      #if !DISABLE_MULTIVIEW
        #extension GL_OVR_multiview : enable
        layout(num_views=NUM_VIEWS) in;
        #define VIEW_ID gl_ViewID_OVR
      #else
        uniform lowp int ViewID;
        #define VIEW_ID ViewID
      #endif
      in vec3 vertexPosition;
      in vec4 vertexColor;
      in mat4 vertexTransform;
      uniform SceneMatrices
      {
         uniform mat4 ViewMatrix[NUM_VIEWS];
         uniform mat4 ProjectionMatrix;
      } sm;
      out vec4 fragmentColor;
      void main()
      {
         gl_Position  = sm.ProjectionMatrix * ( sm.ViewMatrix[VIEW_ID] * ( vertexTransform * vec4( vertexPosition, 1.0 ) ) );
         fragmentColor = vertexColor;
      }`;  */

      var fragmentShaderSource =
        "#version 300 es\n" +
        "out lowp vec4 outColor;\n"+
	"void main()\n"+
	"{\n"+

    "   highp vec2 dx = dFdx(gl_FragCoord.xy), dy = dFdy(gl_FragCoord.xy);\n"+
    "   highp float levelX = floor(0.5*log2(dot(dx, dx)));\n"+
    "   highp float levelY = floor(0.5*log2(dot(dy, dy)));\n" +
    "   lowp vec4 color = vec4(0.0, 0.0, 0.0, 1.0);\n"+
//    "   outColor =  vec4(levelX / 2.0, levelY / 2.0, 1.0, 1.0);\n"+
    "   if ((levelX == 0.0) && (levelY == 0.0)) {\n"+
    "       color = vec4(1.0, 1.0, 1.0, 1.0);\n"+               //  1:1 = White
    "   } else if ((levelX == 1.0) && (levelY == 0.0)) {\n"+
    "       color = vec4(1.0, 0.0, 0.0, 1.0);\n"+               //  1:2 = Red
    "   } else if ((levelX == 1.0) && (levelY == 1.0)) {\n"+
    "       color = vec4(0.0, 1.0, 0.0, 1.0);\n"+               //  1:4 = Green
    "   } else if ((levelX == 2.0) && (levelY == 1.0)) {\n"+
    "       color = vec4(0.0, 0.0, 1.0, 1.0);\n"+               //  1:8 = Blue
    "   } else if ((levelX == 2.0) && (levelY == 2.0)) {\n"+
    "       color = vec4(1.0, 0.0, 1.0, 1.0);\n"+               //  1:16 = Purple
    "   }\n"+
    "	outColor = color;\n"+
	"}\n";
/*
      `#version 300 es
      in lowp vec4 fragmentColor;
      out lowp vec4 outColor;
      void main()
      {
         outColor = fragmentColor;
      }`;  */

console.log(vertexShaderSource);
      var vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
console.log(fragmentShaderSource);
      var fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);
      var program = createProgram(gl, vertexShader, fragmentShader);

      var vertexPositionLocation = gl.getAttribLocation(program, "vertexPosition");
      var vertexColorLocation = gl.getAttribLocation(program, "vertexColor");
      var vertexTransformLocation = gl.getAttribLocation(program, "vertexTransform");

/*      var sceneUniformsLocation = gl.getUniformBlockIndex(program, "SceneMatrices");
      gl.uniformBlockBinding(program, sceneUniformsLocation, 0);
      var modelMatrixLocation = gl.getUniformLocation(program, "ModelMatrix");
      var viewIDLocation = gl.getUniformLocation(program, "ViewID");
  */
      var simulation = new Object;
      simulation.CurrentRotation = vec3.create();

      gl.useProgram(program);

      var scene = createScene(gl);
      //createCube(gl);

      console.log("canvas   " + webglCanvas.width + "x" + webglCanvas.height);
      console.log("gl drbuf " + gl.drawingBufferWidth + "x" + gl.drawingBufferHeight);

      var projMatrix = mat4.create();
      mat4.perspective(projMatrix, Math.PI / 2, gl.drawingBufferWidth / gl.drawingBufferHeight, 0.1, 10000.0);
      var viewMatrix = mat4.create();
      var eyePosition = vec3.fromValues(1, 1, 1);
      mat4.lookAt(viewMatrix, eyePosition, vec3.fromValues(0, 0, 0), vec3.fromValues(0, 1, 0));
      var viewProjMatrix = mat4.create();
      mat4.multiply(viewProjMatrix, projMatrix, viewMatrix);


      function createShader(gl, type, source) {
        var shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        var success = gl.getShaderParameter(shader, gl.COMPILE_STATUS);
        if (success) {
          return shader;
        }

        console.log(gl.getShaderInfoLog(shader));
        gl.deleteShader(shader);
      }

      function createProgram(gl, vertexShader, fragmentShader) {
        var program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        var success = gl.getProgramParameter(program, gl.LINK_STATUS);
        if (success) {
          return program;
        }

        console.log(gl.getProgramInfoLog(program));
        gl.deleteProgram(program);
      }
      // t - time in milliseconds, integer
      function simulationAdvance(t) {
        var rotation = t / 1000.0;
        simulation.CurrentRotation.x = rotation;
        simulation.CurrentRotation.y = rotation * 0.8;
        simulation.CurrentRotation.z = rotation * 1.2;
      }
      function createCube(gl) {
        var cube = new Object;

        const cubeVertices = 
        [
             // positions
             -127, +127, -127, +127 ,  +127, +127, -127, +127 ,  +127, +127, +127, +127 ,  -127, +127, +127, +127 ,	// top
             -127, -127, -127, +127 ,  -127, -127, +127, +127 ,  +127, -127, +127, +127 ,  +127, -127, -127, +127 ,	// bottom
          
            // colors
             255,   0, 255, 255 ,    0, 255,   0, 255 ,    0,   0, 255, 255 ,  255,   0,   0, 255 ,
               0,   0, 255, 255 ,    0, 255,   0, 255 ,  255,   0, 255, 255 ,  255,   0,   0, 255 ,
          
        ];

        cube.VertexCount = 8;
        cube.IndexCount = 36;

        var vertexAttribs = new Array(1);
        vertexAttribs[0] = new Object;
        vertexAttribs[0].index = vertexPositionLocation;
        vertexAttribs[0].size = 4;
        vertexAttribs[0].type = gl.BYTE;
        vertexAttribs[0].normalize = true;
        vertexAttribs[0].stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position
        vertexAttribs[0].offset = 0;        // start at the beginning of the buffer

       /* vertexAttribs[1] = new Object;
        vertexAttribs[1].index = vertexColorLocation;
        vertexAttribs[1].size = 4;
        vertexAttribs[1].type = gl.UNSIGNED_BYTE;
        vertexAttribs[1].normalize = true;
        vertexAttribs[1].stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position
        vertexAttribs[1].offset = 8 * 4;    // skip positions part, offset to color, in bytes
         */
        cube.VertexAttribs = vertexAttribs;

        cube.VertexBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, cube.VertexBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, new Uint8Array(cubeVertices), gl.STATIC_DRAW);
        console.log("cube.VertexBuffer size " + gl.getBufferParameter(gl.ARRAY_BUFFER, gl.BUFFER_SIZE));
        gl.bindBuffer(gl.ARRAY_BUFFER, null);

        const cubeIndices =
        [
          0, 2, 1, 2, 0, 3,	// top
          4, 6, 5, 6, 4, 7,	// bottom
          2, 6, 7, 7, 1, 2,	// right
          0, 4, 5, 5, 3, 0,	// left
          3, 5, 6, 6, 2, 3,	// front
          0, 1, 7, 7, 4, 0	// back
        ];

        cube.IndexBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, cube.IndexBuffer);
        gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(cubeIndices), gl.STATIC_DRAW);
        console.log("cube.IndexBuffer size " + gl.getBufferParameter(gl.ELEMENT_ARRAY_BUFFER, gl.BUFFER_SIZE));
        gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null);

        return cube;
      }

      function GeometryCreateVAO(gl, geometry) {
        geometry.VertexArrayObject = gl.createVertexArray();
        gl.bindVertexArray(geometry.VertexArrayObject);
/*
        gl.bindBuffer(gl.ARRAY_BUFFER, geometry.VertexBuffer);
        for(var i = 0; i < geometry.VertexAttribs.length; ++i) {
          gl.enableVertexAttribArray(geometry.VertexAttribs[i].index);
          gl.vertexAttribPointer(
              geometry.VertexAttribs[i].index, geometry.VertexAttribs[i].size, geometry.VertexAttribs[i].type, 
              geometry.VertexAttribs[i].normalize, geometry.VertexAttribs[i].stride, geometry.VertexAttribs[i].offset);
        }
        gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, geometry.IndexBuffer);    */
        gl.bindVertexArray(null);
      }

      function SceneCreateVAOs(gl, scene)
      {
        if (!scene.CreatedVAOs )
        {
          GeometryCreateVAO(gl, scene.Cube);

          // Modify the VAO to use the instance transform attributes.
          gl.bindVertexArray(scene.Cube.VertexArrayObject);
          gl.bindBuffer(gl.ARRAY_BUFFER, scene.InstanceTransformBuffer);
          /*for (var i = 0; i < 4; ++i)
          {
            gl.enableVertexAttribArray(vertexTransformLocation + i);
            gl.vertexAttribPointer(vertexTransformLocation + i, 4, gl.FLOAT,
                          false, 4 * 4 * 4 , ( i * 4 * 4  ) );
            gl.vertexAttribDivisor(vertexTransformLocation + i, 1 );
          }*/
          gl.bindVertexArray(null);

          scene.CreatedVAOs = true;
        }
      }

      function createScene(gl)
      {
        //ovrProgram_Create( &scene->Program, VERTEX_SHADER, FRAGMENT_SHADER, useMultiview );
        var obj = new Object;

        obj.Cube = createCube(gl);
        //var instArr = new Float32Array(NUM_INSTANCES*4*4);
        // Create the instance transform attribute buffer.
        obj.InstanceTransformBuffer = gl.createBuffer(); 
        gl.bindBuffer(gl.ARRAY_BUFFER, obj.InstanceTransformBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, NUM_INSTANCES * 4 * 4 * 4, gl.DYNAMIC_DRAW);
        console.log("InstanceTransformBuffer size " + gl.getBufferParameter(gl.ARRAY_BUFFER, gl.BUFFER_SIZE));
        gl.bindBuffer(gl.ARRAY_BUFFER, null);

        obj.SceneMatrices = gl.createBuffer(); 
        gl.bindBuffer(gl.UNIFORM_BUFFER, obj.SceneMatrices);
        gl.bufferData(gl.UNIFORM_BUFFER, 2 * 16 * 4 /*sizeof( ovrMatrix4f )*/ /* 2 view matrices */ + 2 * 16 * 4 /*sizeof( ovrMatrix4f )*/ /* 2 projection matrices */, gl.STATIC_DRAW);
        gl.bindBuffer(gl.UNIFORM_BUFFER, null);

        // Setup random rotations.
        obj.Rotations = new Array(NUM_ROTATIONS);
        for ( var i = 0; i < NUM_ROTATIONS; i++ )
        {
          obj.Rotations[i] = new Object;
          obj.Rotations[i].x = Math.random();
          obj.Rotations[i].y = Math.random();
          obj.Rotations[i].z = Math.random();
        }

        // Setup random cube positions and rotations.
        obj.CubePositions = new Array(NUM_INSTANCES);
        obj.CubeRotations = new Int32Array(NUM_INSTANCES);
        for ( var i = 0; i < NUM_INSTANCES; i++ )
        {
          obj.CubePositions[i] = { x:0, y:0, z:0 };
          // Using volatile keeps the compiler from optimizing away multiple calls to ovrScene_RandomFloat().
          var rx, ry, rz;
          for ( ; ; )
          {
            rx = ( Math.random() - 0.5 ) * ( 50.0 + Math.sqrt( NUM_INSTANCES ) );
            ry = ( Math.random() - 0.5 ) * ( 50.0 + Math.sqrt( NUM_INSTANCES ) );
            rz = ( Math.random() - 0.5 ) * ( 50.0 + Math.sqrt( NUM_INSTANCES ) );
            // If too close to 0,0,0
            if ( Math.abs( rx ) < 4.0 && Math.abs( ry ) < 4.0 && Math.abs( rz ) < 4.0 )
            {
              continue;
            }
            // Test for overlap with any of the existing cubes.
            var overlap = false;
            for ( var j = 0; j < i; j++ )
            {
              if (Math.abs( rx - obj.CubePositions[j].x ) < 4.0 &&
                  Math.abs( ry - obj.CubePositions[j].y ) < 4.0 &&
                  Math.abs( rz - obj.CubePositions[j].z ) < 4.0 )
              {
                overlap = true;
                break;
              }
            }
            if ( !overlap )
            {
              break;
            }
          }

          // Insert into list sorted based on distance.
          var insert = 0;
          const distSqr = rx * rx + ry * ry + rz * rz;
          for ( var j = i; j > 0; j-- )
          {
            var otherPos = obj.CubePositions[j - 1];
            const otherDistSqr = otherPos.x * otherPos.x + otherPos.y * otherPos.y + otherPos.z * otherPos.z;
            if ( distSqr > otherDistSqr )
            {
              insert = j;
              break;
            }
            obj.CubePositions[j] = Object.assign({}, obj.CubePositions[j - 1]);
            obj.CubeRotations[j] = obj.CubeRotations[j - 1];
          }

          obj.CubePositions[insert].x = rx;
          obj.CubePositions[insert].y = ry;
          obj.CubePositions[insert].z = rz;

          obj.CubeRotations[insert] = Math.trunc( Math.random() * ( NUM_ROTATIONS - 0.1 ) );
        }

        obj.CreatedScene = true;

        SceneCreateVAOs(gl, obj);
        return obj;
      }

      var fovL = false;
      function onClick () {
        // Reset the background color to a random value
        gl.clearColor(
            Math.random() * 0.5,
            Math.random() * 0.5,
            Math.random() * 0.5, 1.0);

        fovL = !fovL;
        var attributes = {
          depth: true,
          antialias: true,
          multiview: multiviewSupported,
          foveationLevel: ((fovL) ? 3 : 0)
        };
        console.log("onClick");
        vrDisplay.requestPresent([{ source: webglCanvas, attributes: attributes}]).then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "requestPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
          console.log("onVRRequestPresent: errMsg");
        });

      }
      webglCanvas.addEventListener("click", onClick, false);
      function renderFrame(gl, leftVP, leftProjMatrix, leftViewMatrix, rightVP, rightProjMatrix, rightViewMatrix, time, numBuffers) 
      {
        const vps = { 0:leftVP, 1:rightVP };
        gl.clearColor( 0.125, 0.0, 0.125, 1.0);

        // Render the eye images.
        for (var eye = 0; eye < numBuffers; ++eye )
        {
          gl.viewport(vps[eye].x, vps[eye].y, vps[eye].width, vps[eye].height);
          gl.scissor(vps[eye].x, vps[eye].y, vps[eye].width, vps[eye].height);
          gl.disable( gl.SCISSOR_TEST );
          gl.depthMask(false);
          gl.disable( gl.DEPTH_TEST );
          gl.depthFunc( gl.LEQUAL );
          gl.clear( gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT );

          // NOTE: In the non-mv case, latency can be further reduced by updating the sensor prediction
          // for each eye (updates orientation, not position)
          //ovrFramebuffer * frameBuffer = &renderer->FrameBuffer[eye];
          //ovrFramebuffer_SetCurrent( frameBuffer );

          gl.useProgram(program);

/*          gl.bindBufferBase( gl.UNIFORM_BUFFER, sceneUniformsLocation, scene.SceneMatrices );
          if (viewIDLocation != null)  // NOTE: will not be present when multiview path enabled.
          {
            gl.uniform1i(viewIDLocation, eye);
          }*/
          gl.disable( gl.SCISSOR_TEST );
          gl.enable( gl.CULL_FACE );
          gl.cullFace( gl.BACK );
          //gl.viewport( 0, 0, frameBuffer->Width, frameBuffer->Height ) );
          //gl.scissor( 0, 0, frameBuffer->Width, frameBuffer->Height ) );
          gl.bindVertexArray( scene.Cube.VertexArrayObject );
//          gl.drawElements(gl.TRIANGLES, scene.Cube.IndexCount, gl.UNSIGNED_SHORT, 0);
          gl.drawArrays(gl.TRIANGLES, 0, 6)
          gl.bindVertexArray(null);
          gl.useProgram(null);
        }
      }


      // ================================
      // WebVR-specific code begins here.
      // ================================
      function onVRRequestPresent () {
        console.log("onVRRequestPresent begin");
        // This can only be called in response to a user gesture.
        var attributes = {
          depth: true,
          antialias: true,
          multiview: multiviewSupported,
        };
        vrDisplay.requestPresent([{ source: webglCanvas, attributes: attributes}]).then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "requestPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
          console.log("onVRRequestPresent: errMsg");
        });
        console.log("onVRRequestPresent end");
      }

      function onVRExitPresent () {
        // No sense in exiting presentation if we're not actually presenting.
        // (This may happen if we get an event like vrdisplaydeactivate when
        // we weren't presenting.)
        if (!vrDisplay.isPresenting)
          return;

        vrDisplay.exitPresent().then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "exitPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
        });
      }

      function onVRPresentChange () {
        console.log("onVRPresentChange begin");
        // When we begin or end presenting, the canvas should be resized to the
        // recommended dimensions for the display.
        onResize();

        if (vrDisplay.isPresenting) {
          var views = vrDisplay.getViews ? vrDisplay.getViews() : [];
          if (views.length > 0) {
            var view = views[0];
            is_multiview = view.getAttributes().multiview;
            console.log("onVRPresentChange, presenting, multiview = " + is_multiview);
          }
          if (vrDisplay.capabilities.hasExternalDisplay) {
            // Because we're not mirroring any images on an external screen will
            // freeze while presenting. It's better to replace it with a message
            // indicating that content is being shown on the VRDisplay.
            presentingMessage.style.display = "block";

            // On devices with an external display the UA may not provide a way
            // to exit VR presentation mode, so we should provide one ourselves.
            VRSamplesUtil.removeButton(vrPresentButton);
            vrPresentButton = VRSamplesUtil.addButton("Exit VR", "E", "media/icons/cardboard64.png", onVRExitPresent);
          }
        } else {
          // If we have an external display take down the presenting message and
          // change the button back to "Enter VR".
          if (vrDisplay.capabilities.hasExternalDisplay) {
            presentingMessage.style.display = "";

            VRSamplesUtil.removeButton(vrPresentButton);
            vrPresentButton = VRSamplesUtil.addButton("Enter VR", "E", "media/icons/cardboard64.png", onVRRequestPresent);
          }
        }
        // When we begin or end presenting, the canvas should be resized to the
        // recommended dimensions for the display.
        onResize();
        console.log("onVRPresentChange end");
      }

      if (navigator.getVRDisplays) {
        frameData = new VRFrameData();

        navigator.getVRDisplays().then(function (displays) {
          if (displays.length > 0) {
            vrDisplay = displays[displays.length - 1];

            // It's heighly reccommended that you set the near and far planes to
            // something appropriate for your scene so the projection matricies
            // WebVR produces have a well scaled depth buffer.
            vrDisplay.depthNear = 0.1;
            vrDisplay.depthFar = 1024.0;

            VRSamplesUtil.addButton("Reset Pose", "R", null, function () { vrDisplay.resetPose(); });

            // Generally, you want to wait until VR support is confirmed and
            // you know the user has a VRDisplay capable of presenting connected
            // before adding UI that advertises VR features.
            if (vrDisplay.capabilities.canPresent)
              vrPresentButton = VRSamplesUtil.addButton("Enter VR", "E", "media/icons/cardboard64.png", onVRRequestPresent);

            // The UA may kick us out of VR present mode for any reason, so to
            // ensure we always know when we begin/end presenting we need to
            // listen for vrdisplaypresentchange events.
            window.addEventListener('vrdisplaypresentchange', onVRPresentChange, false);

            // These events fire when the user agent has had some indication that
            // it would be appropariate to enter or exit VR presentation mode, such
            // as the user putting on a headset and triggering a proximity sensor.
            // You can inspect the `reason` property of the event to learn why the
            // event was fired, but in this case we're going to always trust the
            // event and enter or exit VR presentation mode when asked.
            window.addEventListener('vrdisplayactivate', onVRRequestPresent, false);
            window.addEventListener('vrdisplaydeactivate', onVRExitPresent, false);
          } else {
            VRSamplesUtil.addInfo("WebVR supported, but no VRDisplays found.", 3000);
          }
        });
      } else if (navigator.getVRDevices) {
        VRSamplesUtil.addError("Your browser supports WebVR but not the latest version. See <a href='http://webvr.info'>webvr.info</a> for more info.");
      } else {
        VRSamplesUtil.addError("Your browser does not support WebVR. See <a href='http://webvr.info'>webvr.info</a> for assistance.");
      }

      var resolutionMultiplier = 1.0;
      var eyeWidth, eyeHeight;
      var lastAdjustment = Date.now()-1000;

      function adjustResolution() {
        var t = Date.now();
        // Update the resolution every tenth of a second
        if (t - lastAdjustment < 100)
          return;
        lastAdjustment = t;

        // Modify the resolution we are rendering at over time on a sin wave.
        // In the real world this would probably be based on scene complexity.
        // Oscillates between 1.0 to 0.2.
        resolutionMultiplier = (Math.sin(t / 1000) * 0.4) + 0.6;

        eyeWidth = Math.floor(webglCanvas.width * 0.5 * resolutionMultiplier);
        eyeHeight = Math.floor(webglCanvas.height * resolutionMultiplier);

        // Layer bounds are described in UV space, so 0.0 to 1.0
        var boundsWidth = eyeWidth / webglCanvas.width;
        var boundsHeight = eyeHeight / webglCanvas.height;

        // Tell the presenting display about the new texture bounds. This
        // ensures it only picks up the parts of the texture we're going to be
        // rendering to and avoids the need to resize the WebGL canvas, which
        // can be a slow operation. Because we're already presenting when we
        // call requestPresent again it only updates the VRLayer information and
        // doesn't require a user gesture.
/*        vrDisplay.requestPresent([{
          source: webglCanvas,
          leftBounds: [0.0, 0.0, boundsWidth, boundsHeight],
          rightBounds: [boundsWidth, 0.0, boundsWidth, boundsHeight],
        }]);                                                         */

        vrDisplay.requestPresent([{
          source: webglCanvas,
          leftBounds: [0.25 - boundsWidth*0.5, 0.5 - boundsHeight*0.5, boundsWidth, boundsHeight],
          rightBounds: [0.5 + 0.25 - boundsWidth*0.5, 0.5 - boundsHeight*0.5, boundsWidth, boundsHeight],
        }]);


        if (vrDisplay.capabilities.hasExternalDisplay) {
          // To ensure our mirrored content also shows up correctly we'll scale
          // the canvas display size to be scaled appropriately such that it
          // continues to only show one eye.
          webglCanvas.style.width = (1.0/resolutionMultiplier) * 200 + "%";
          webglCanvas.style.height = (1.0/resolutionMultiplier) * 100 + "%";
        }
      }

      function onResize () {
        if (vrDisplay && vrDisplay.isPresenting) {
          if (!is_multiview) {
            // If we're presenting we want to use the drawing buffer size
            // recommended by the VRDevice, since that will ensure the best
            // results post-distortion.
            var leftEye = vrDisplay.getEyeParameters("left");
            var rightEye = vrDisplay.getEyeParameters("right");

            // For simplicity we're going to render both eyes at the same size,
            // even if one eye needs less resolution. You can render each eye at
            // the exact size it needs, but you'll need to adjust the viewports to
            // account for that.
            webglCanvas.width = Math.max(leftEye.renderWidth, rightEye.renderWidth) * 2;
            webglCanvas.height = Math.max(leftEye.renderHeight, rightEye.renderHeight);
            console.log("onResize, presenting, multiview = " + is_multiview + ", new size = " + webglCanvas.width + "x" + webglCanvas.height);
          }
          else {
            console.log("onResize is skipped, multiview = " + is_multiview);
          }
        } else {
          // We only want to change the size of the canvas drawing buffer to
          // match the window dimensions when we're not presenting.
          webglCanvas.width = webglCanvas.offsetWidth * window.devicePixelRatio;
          webglCanvas.height = webglCanvas.offsetHeight * window.devicePixelRatio;
        }
      }
      window.addEventListener("resize", onResize, false);
      onResize();

      function eventFired(evt) {
        VRSamplesUtil.addInfo("[" + evt.type + "] VR Display: " + evt.display.displayName + ", Reason: " + evt.reason, 3000);
        console.log("[" + evt.type + "] VR Display: " + evt.display.displayName + ", Reason: " + evt.reason);
      }

      var reported = false;
      var button_pressed = new Array(10);
      function onAnimationFrame (t) {
        gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

          // Loop over every gamepad and if we find any that have a pose use it.
          var vrGamepads = [];

          var gamepads = navigator.getGamepads();
          for (var i = 0; i < gamepads.length; ++i) {
            var gamepad = gamepads[i];
            // The array may contain undefined gamepads, so check for that as
            // well as a non-null pose.
            if (gamepad) {
              if (gamepad.pose)
                vrGamepads.push(gamepad);

                for (var j = 0; j < gamepad.buttons.length; ++j) {
                  if (gamepad.buttons[j].pressed) {
                    if (!button_pressed[j]) {
                      console.log("Button " + j + " pressed");
                      button_pressed[j] = true;
                      onClick();
                    }
                  }
                  else {
                    button_pressed[j] = false;
                  }
               }
            }
          }

        if (vrDisplay) {
          window.addEventListener("vrdisplayconnect", eventFired, false);
          window.addEventListener("vrdisplaydisconnect", eventFired, false);
          window.addEventListener("vrdisplayactivate", eventFired, false);
          window.addEventListener("vrdisplaydeactivate", eventFired, false);
          window.addEventListener("vrdisplayblur", eventFired, false);
          window.addEventListener("vrdisplayfocus", eventFired, false);
          // When presenting content to the VRDisplay we want to update at its
          // refresh rate if it differs from the refresh rate of the main
          // display. Calling VRDisplay.requestAnimationFrame ensures we render
          // at the right speed for VR.
          vrDisplay.requestAnimationFrame(onAnimationFrame);

          // As a general rule you want to get the pose as late as possible
          // and call VRDisplay.submitFrame as early as possible after
          // retrieving the pose. Do any work for the frame that doesn't need
          // to know the pose earlier to ensure the lowest latency possible.
          //var pose = vrDisplay.getPose();
          vrDisplay.getFrameData(frameData);

          //          console.log("vrDisplay.isPresenting = " + vrDisplay.isPresenting + " fov = " + (Math.atan2(1.0, frameData.leftProjectionMatrix[5]) * 360 / Math.PI));
          if (vrDisplay.isPresenting) {
            adjustResolution();
            // When presenting render a stereo view.

            var views = vrDisplay.getViews ? vrDisplay.getViews() : [];
            //console.log("views: " + vrDisplay.getViews);
            if (views && views.length > 0) {
              if (views[0].getAttributes().multiview) {
                // We should have only one view in MV case
                if (!reported) {
                  console.log("Rendering with multiview!");
                  reported = true;
                }
                var view = views[0];
                var viewport = view.getViewport();
                gl.bindFramebuffer(gl.FRAMEBUFFER, view.framebuffer);
                renderFrame(gl, viewport, frameData.leftProjectionMatrix, frameData.leftViewMatrix, viewport, frameData.rightProjectionMatrix, frameData.rightViewMatrix, t, 1);
              }
              else {
                //renderFrame(gl, viewports[0], frameData.leftProjectionMatrix, frameData.leftViewMatrix, viewports[1], frameData.rightProjectionMatrix, frameData.rightViewMatrix, t, 2);
                var VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
                renderFrame(gl, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, t, 1);
              }

              // If we're currently presenting to the VRDisplay we need to
              // explicitly indicate we're done rendering.
              vrDisplay.submitFrame();
            }
            else {
                var VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
                renderFrame(gl, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, t, 1);

              // If we're currently presenting to the VRDisplay we need to
              // explicitly indicate we're done rendering.
              vrDisplay.submitFrame();
            }
          } else {
            //window.requestAnimationFrame(onAnimationFrame);
            // When not presenting render a mono view that still takes pose into
            // account.
            var VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
            renderFrame(gl, VP, projMatrix, viewMatrix, VP, projMatrix, viewMatrix, t, 1);

            reported = false;
          }
        } else {
          //console.log("vrDisplay = " + vrDisplay);
          window.requestAnimationFrame(onAnimationFrame);

          // No VRDisplay found.
          var VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
          renderFrame(gl, VP, projMatrix, viewMatrix, VP, projMatrix, viewMatrix, t, 1);
        }
        simulationAdvance(t);
      }


      window.requestAnimationFrame(onAnimationFrame);
    

      })();
    </script>
  </body>
</html>
