<!doctype html>
<!--
Copyright 2016 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">

    <title>FOVEATION</title>
    <style>
      #webgl-canvas, #presenting-message {
        box-sizing: border-box;
        height: 100%;
        left: 0;
        margin: 0;
        position: absolute;
        top: 0;
        width: 100%;
      }
      #presenting-message {
        color: white;
        font-family: sans-serif;
        font-size: 2em;
        font-weight: bold;
        z-index: 1;
        text-align: center;
        padding: 0.5em;
        background-color: #444;
        display: none;
      }
    </style>
    <script src="js/third-party/gl-matrix.js"></script>

    <script src="js/third-party/wglu/wglu-program.js"></script>
    <script src="js/third-party/wglu/wglu-stats.js"></script>
    <script src="js/third-party/wglu/wglu-texture.js"></script>

    <script src="js/vr-cube-sea.js"></script>
    <script src="js/vr-samples-util.js"></script>
  </head>
  <body>
    <canvas id="webgl-canvas"></canvas>
    <script>
      (function () {
      "use strict";

      var vrDisplay = null;
      const NUM_INSTANCES = 1;
      const NUM_ROTATIONS = 16;
      var multiview = 0;
      var frameData = null;

      var vrPresentButton = null;
      var is_multiview = false;
      var multiviewSupported = false;

      var webglCanvas = document.getElementById("webgl-canvas");
      var glAttribs = {
        alpha: true,
        depth: true,
        stencil: true,
        preserveDrawingBuffer : false,
        antialias : true,
      };
      var gl = null;
      gl = webglCanvas.getContext("webgl", glAttribs);
      if (!gl) {
        VRSamplesUtil.addError("Can't crate WebGL context");
        return;
      }

      gl.clearColor(0.1, 0.2, 0.3, 1.0);
      gl.enable(gl.DEPTH_TEST);
      gl.enable(gl.CULL_FACE);


      var simulation = new Object;
      simulation.CurrentRotation = vec3.create();


      //var scene = createScene(gl);
      //createCube(gl);

      console.log("canvas   " + webglCanvas.width + "x" + webglCanvas.height);
      console.log("gl drbuf " + gl.drawingBufferWidth + "x" + gl.drawingBufferHeight);

      var projMatrix = mat4.create();
      mat4.perspective(projMatrix, Math.PI / 2, gl.drawingBufferWidth / gl.drawingBufferHeight, 0.1, 10000.0);
      var viewMatrix = mat4.create();
      var eyePosition = vec3.fromValues(1, 1, 1);
      mat4.lookAt(viewMatrix, eyePosition, vec3.fromValues(0, 0, 0), vec3.fromValues(0, 1, 0));
      var viewProjMatrix = mat4.create();
      mat4.multiply(viewProjMatrix, projMatrix, viewMatrix);


      function createShader(gl, type, source) {
        var shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        var success = gl.getShaderParameter(shader, gl.COMPILE_STATUS);
        if (success) {
          return shader;
        }

        console.log(gl.getShaderInfoLog(shader));
        gl.deleteShader(shader);
      }

      function createProgram(gl, vertexShader, fragmentShader) {
        var program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        var success = gl.getProgramParameter(program, gl.LINK_STATUS);
        if (success) {
          return program;
        }

        console.log(gl.getProgramInfoLog(program));
        gl.deleteProgram(program);
      }
      // t - time in milliseconds, integer
      function simulationAdvance(t) {
        var rotation = t / 1000.0;
        simulation.CurrentRotation.x = rotation;
        simulation.CurrentRotation.y = rotation * 0.8;
        simulation.CurrentRotation.z = rotation * 1.2;
      }
      function GeometryCreateVAO(gl, geometry) {
        geometry.VertexArrayObject = gl.createVertexArray();
        gl.bindVertexArray(geometry.VertexArrayObject);
        gl.bindVertexArray(null);
      }

      function SceneCreateVAOs(gl, scene)
      {
        if (!scene.CreatedVAOs )
        {
          GeometryCreateVAO(gl, scene.Cube);

          // Modify the VAO to use the instance transform attributes.
          gl.bindVertexArray(scene.Cube.VertexArrayObject);
          gl.bindBuffer(gl.ARRAY_BUFFER, scene.InstanceTransformBuffer);
          gl.bindVertexArray(null);

          scene.CreatedVAOs = true;
        }
      }


      var fovL = false;
      function onClick () {
        // Reset the background color to a random value
        gl.clearColor(
            Math.random() * 0.5,
            Math.random() * 0.5,
            Math.random() * 0.5, 1.0);

        fovL = !fovL;
        var attributes = {
          depth: true,
          antialias: true,
          multiview: multiviewSupported,
          foveationLevel: ((fovL) ? 3 : 0)
        };
        console.log("onClick");
        vrDisplay.requestPresent([{ source: webglCanvas, attributes: attributes}]).then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "requestPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
          console.log("onVRRequestPresent: errMsg");
        });

      }
      webglCanvas.addEventListener("click", onClick, false);
      function renderFrame(gl, leftVP, leftProjMatrix, leftViewMatrix, rightVP, rightProjMatrix, rightViewMatrix, time, numBuffers) 
      {
        const vps = { 0:leftVP, 1:rightVP };
        gl.clearColor( 0.125, 0.1, 0.625, 1.0);

        // Render the eye images.
        for (var eye = 0; eye < numBuffers; ++eye )
        {
          gl.viewport(vps[eye].x, vps[eye].y, vps[eye].width, vps[eye].height);



          const vs = `
          attribute vec4 position;
          uniform mat4 matrix;
          void main() {
            gl_Position = matrix * position;
          }
          `;

          const fs = `
          precision mediump float;
          uniform vec4 color;
          void main() {
            gl_FragColor = color;
          }
          `;
          const vertexShader = createShader(gl, gl.VERTEX_SHADER, vs);
          const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fs);
          const program = createProgram(gl, vertexShader, fragmentShader);

          const posLoc = gl.getAttribLocation(program, 'position');
          const matLoc = gl.getUniformLocation(program, 'matrix');
          const colorLoc = gl.getUniformLocation(program, 'color');

          const buf = gl.createBuffer();
          gl.bindBuffer(gl.ARRAY_BUFFER, buf);
          gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([
             0, -1,
             1,  1,
            -1,  1,
          ]), gl.STATIC_DRAW);

          gl.enableVertexAttribArray(posLoc);
          gl.vertexAttribPointer(
              posLoc,    // attribute location
              2,         // 2 value per vertex
              gl.FLOAT,  // 32bit floating point values
              false,     // don't normalize
              0,         // stride (0 = base on type and size)
              0,         // offset into buffer
          );

          // clear the stencil to 0 (the default)
          gl.scissor(vps[eye].x, vps[eye].y, vps[eye].width, vps[eye].height);
          gl.enable( gl.SCISSOR_TEST );
          gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT | gl.STENCIL_BUFFER_BIT);
//          gl.disable( gl.SCISSOR_TEST );
          gl.disable( gl.DEPTH_TEST );
          gl.stencilMask(0xFF);

          gl.useProgram(program);
          let m4 = mat4.create();


          // turn on the stencil
          gl.enable(gl.STENCIL_TEST);

          // Set the stencil test so it always passes
          // and the reference to 1
          gl.stencilFunc(
             gl.ALWAYS,    // the test
             1,            // reference value
             0xFF,         // mask
          );
          // Set it so we replace with the reference value (1)
          gl.stencilOp(
             gl.KEEP,     // what to do if the stencil test fails
             gl.KEEP,     // what to do if the depth test fails
             gl.REPLACE,  // what to do if both tests pass
          );
                                     /*
          mat4.fromScaling(m4, [0.9, 0.9, 1]);
          //mat4.translate(m4, m4, [1,1,0]);
          // draw a large green triangle            
          gl.uniform4fv(colorLoc, [0, 1, 0, 1]); // green
          gl.uniformMatrix4fv(matLoc, false, m4);
          gl.drawArrays(gl.TRIANGLES, 0, 3);
                                       */

          
          mat4.fromScaling(m4, [0.2, 0.8, 1]);
          //mat4.translate(m4, m4, [1,1,0]);
          // draw a white small triangle
          gl.uniform4fv(colorLoc, [1, 1, 1, 1]); // white
          gl.uniformMatrix4fv(matLoc, false, m4);
          gl.colorMask(gl.FALSE, gl.FALSE, gl.FALSE, gl.FALSE);
          gl.drawArrays(gl.TRIANGLES, 0, 3);

          gl.colorMask(true, true, true, true);

          // Set the test that the stencil must = 0
          gl.stencilFunc(
             gl.EQUAL,     // the test
             0,            // reference value
             0xFF,         // mask
          );
          // don't change the stencil buffer on draw
          gl.stencilOp(
             gl.KEEP,     // what to do if the stencil test fails
             gl.KEEP,     // what to do if the depth test fails
             gl.KEEP,  // what to do if both tests pass
          );
          gl.stencilMask(0);

          mat4.fromScaling(m4, [0.9, 0.9, 1]);
          //mat4.translate(m4, m4, [1,1,0]);
          // draw a large green triangle            
          gl.uniform4fv(colorLoc, [0, 1, 0, 1]); // green
          gl.uniformMatrix4fv(matLoc, false, m4);
          gl.drawArrays(gl.TRIANGLES, 0, 3);

          /*
          gl.scissor(vps[eye].x, vps[eye].y, vps[eye].width, vps[eye].height);
          gl.disable( gl.SCISSOR_TEST );
          gl.depthMask(false);
          gl.disable( gl.DEPTH_TEST );
          gl.depthFunc( gl.LEQUAL );
          gl.clear( gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT );

          // NOTE: In the non-mv case, latency can be further reduced by updating the sensor prediction
          // for each eye (updates orientation, not position)
          //ovrFramebuffer * frameBuffer = &renderer->FrameBuffer[eye];
          //ovrFramebuffer_SetCurrent( frameBuffer );

          gl.useProgram(program);

          gl.disable( gl.SCISSOR_TEST );
          gl.enable( gl.CULL_FACE );
          gl.cullFace( gl.BACK );
          //gl.viewport( 0, 0, frameBuffer->Width, frameBuffer->Height ) );
          //gl.scissor( 0, 0, frameBuffer->Width, frameBuffer->Height ) );
          gl.bindVertexArray( scene.Cube.VertexArrayObject );

          gl.drawArrays(gl.TRIANGLES, 0, 6)
          gl.bindVertexArray(null);
          gl.useProgram(null); */
        }
      }


      // ================================
      // WebVR-specific code begins here.
      // ================================
      function onVRRequestPresent () {
        console.log("onVRRequestPresent begin");
        // This can only be called in response to a user gesture.
        var attributes = {
          depth: true,
          antialias: true,
          multiview: multiviewSupported,
        };
        vrDisplay.requestPresent([{ source: webglCanvas, attributes: attributes}]).then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "requestPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
          console.log("onVRRequestPresent: errMsg");
        });
        console.log("onVRRequestPresent end");
      }

      function onVRExitPresent () {
        // No sense in exiting presentation if we're not actually presenting.
        // (This may happen if we get an event like vrdisplaydeactivate when
        // we weren't presenting.)
        if (!vrDisplay.isPresenting)
          return;

        vrDisplay.exitPresent().then(function () {
          // Nothing to do because we're handling things in onVRPresentChange.
        }, function (err) {
          var errMsg = "exitPresent failed.";
          if (err && err.message) {
            errMsg += "<br/>" + err.message
          }
          VRSamplesUtil.addError(errMsg, 2000);
        });
      }

      function onVRPresentChange () {
        console.log("onVRPresentChange begin");
        // When we begin or end presenting, the canvas should be resized to the
        // recommended dimensions for the display.
        onResize();

        if (vrDisplay.isPresenting) {
          var views = vrDisplay.getViews ? vrDisplay.getViews() : [];
          if (views.length > 0) {
            var view = views[0];
            is_multiview = view.getAttributes().multiview;
            console.log("onVRPresentChange, presenting, multiview = " + is_multiview);
          }
          if (vrDisplay.capabilities.hasExternalDisplay) {
            // Because we're not mirroring any images on an external screen will
            // freeze while presenting. It's better to replace it with a message
            // indicating that content is being shown on the VRDisplay.
            presentingMessage.style.display = "block";

            // On devices with an external display the UA may not provide a way
            // to exit VR presentation mode, so we should provide one ourselves.
            VRSamplesUtil.removeButton(vrPresentButton);
            vrPresentButton = VRSamplesUtil.addButton("Exit VR", "E", "media/icons/cardboard64.png", onVRExitPresent);
          }
        } else {
          // If we have an external display take down the presenting message and
          // change the button back to "Enter VR".
          if (vrDisplay.capabilities.hasExternalDisplay) {
            presentingMessage.style.display = "";

            VRSamplesUtil.removeButton(vrPresentButton);
            vrPresentButton = VRSamplesUtil.addButton("Enter VR", "E", "media/icons/cardboard64.png", onVRRequestPresent);
          }
        }
        // When we begin or end presenting, the canvas should be resized to the
        // recommended dimensions for the display.
        onResize();
        console.log("onVRPresentChange end");
      }

      if (navigator.getVRDisplays) {
        frameData = new VRFrameData();

        navigator.getVRDisplays().then(function (displays) {
          if (displays.length > 0) {
            vrDisplay = displays[displays.length - 1];

            // It's heighly reccommended that you set the near and far planes to
            // something appropriate for your scene so the projection matricies
            // WebVR produces have a well scaled depth buffer.
            vrDisplay.depthNear = 0.1;
            vrDisplay.depthFar = 1024.0;

            VRSamplesUtil.addButton("Reset Pose", "R", null, function () { vrDisplay.resetPose(); });

            // Generally, you want to wait until VR support is confirmed and
            // you know the user has a VRDisplay capable of presenting connected
            // before adding UI that advertises VR features.
            if (vrDisplay.capabilities.canPresent)
              vrPresentButton = VRSamplesUtil.addButton("Enter VR", "E", "media/icons/cardboard64.png", onVRRequestPresent);

            // The UA may kick us out of VR present mode for any reason, so to
            // ensure we always know when we begin/end presenting we need to
            // listen for vrdisplaypresentchange events.
            window.addEventListener('vrdisplaypresentchange', onVRPresentChange, false);

            // These events fire when the user agent has had some indication that
            // it would be appropariate to enter or exit VR presentation mode, such
            // as the user putting on a headset and triggering a proximity sensor.
            // You can inspect the `reason` property of the event to learn why the
            // event was fired, but in this case we're going to always trust the
            // event and enter or exit VR presentation mode when asked.
            window.addEventListener('vrdisplayactivate', onVRRequestPresent, false);
            window.addEventListener('vrdisplaydeactivate', onVRExitPresent, false);
          } else {
            VRSamplesUtil.addInfo("WebVR supported, but no VRDisplays found.", 3000);
          }
        });
      } else if (navigator.getVRDevices) {
        VRSamplesUtil.addError("Your browser supports WebVR but not the latest version. See <a href='http://webvr.info'>webvr.info</a> for more info.");
      } else {
        VRSamplesUtil.addError("Your browser does not support WebVR. See <a href='http://webvr.info'>webvr.info</a> for assistance.");
      }

      function onResize () {
        if (vrDisplay && vrDisplay.isPresenting) {
          if (!is_multiview) {
            // If we're presenting we want to use the drawing buffer size
            // recommended by the VRDevice, since that will ensure the best
            // results post-distortion.
            var leftEye = vrDisplay.getEyeParameters("left");
            var rightEye = vrDisplay.getEyeParameters("right");

            // For simplicity we're going to render both eyes at the same size,
            // even if one eye needs less resolution. You can render each eye at
            // the exact size it needs, but you'll need to adjust the viewports to
            // account for that.
            webglCanvas.width = Math.max(leftEye.renderWidth, rightEye.renderWidth) * 2;
            webglCanvas.height = Math.max(leftEye.renderHeight, rightEye.renderHeight);
            console.log("onResize, presenting, multiview = " + is_multiview + ", new size = " + webglCanvas.width + "x" + webglCanvas.height);
          }
          else {
            console.log("onResize is skipped, multiview = " + is_multiview);
          }
        } else {
          // We only want to change the size of the canvas drawing buffer to
          // match the window dimensions when we're not presenting.
          webglCanvas.width = webglCanvas.offsetWidth * window.devicePixelRatio;
          webglCanvas.height = webglCanvas.offsetHeight * window.devicePixelRatio;
        }
      }
      window.addEventListener("resize", onResize, false);
      onResize();

      function eventFired(evt) {
        VRSamplesUtil.addInfo("[" + evt.type + "] VR Display: " + evt.display.displayName + ", Reason: " + evt.reason, 3000);
        console.log("[" + evt.type + "] VR Display: " + evt.display.displayName + ", Reason: " + evt.reason);
      }

      var reported = false;
      var button_pressed = [new Array(10), new Array(10)];
      function onAnimationFrame (t) {
        gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

        let gamepads = navigator.getGamepads();
        for (var i = 0; i < gamepads.length; ++i) {
          let gamepad = gamepads[i];
          // The array may contain undefined gamepads, so check for that as
          // well as a non-null pose.
          if (gamepad) {
            if (gamepad.pose)
              for (let j = 0; j < gamepad.buttons.length; ++j) {
                if (gamepad.buttons[j].pressed) {
                  if (!button_pressed[i][j]) {
                    console.log("Gamepad " + i + ", Button " + j + " pressed");
                    button_pressed[i][j] = true;
                    onClick();
                  }
                }
                else {
                  button_pressed[i][j] = false;
                }
             }
          }
        }

        if (vrDisplay) {
          window.addEventListener("vrdisplayconnect", eventFired, false);
          window.addEventListener("vrdisplaydisconnect", eventFired, false);
          window.addEventListener("vrdisplayactivate", eventFired, false);
          window.addEventListener("vrdisplaydeactivate", eventFired, false);
          window.addEventListener("vrdisplayblur", eventFired, false);
          window.addEventListener("vrdisplayfocus", eventFired, false);
          // When presenting content to the VRDisplay we want to update at its
          // refresh rate if it differs from the refresh rate of the main
          // display. Calling VRDisplay.requestAnimationFrame ensures we render
          // at the right speed for VR.
          vrDisplay.requestAnimationFrame(onAnimationFrame);

          // As a general rule you want to get the pose as late as possible
          // and call VRDisplay.submitFrame as early as possible after
          // retrieving the pose. Do any work for the frame that doesn't need
          // to know the pose earlier to ensure the lowest latency possible.
          //var pose = vrDisplay.getPose();
          vrDisplay.getFrameData(frameData);

          //          console.log("vrDisplay.isPresenting = " + vrDisplay.isPresenting + " fov = " + (Math.atan2(1.0, frameData.leftProjectionMatrix[5]) * 360 / Math.PI));
          if (vrDisplay.isPresenting) {
            // When presenting render a stereo view.

            let views = vrDisplay.getViews ? vrDisplay.getViews() : [];
            //console.log("views: " + vrDisplay.getViews);
            if (views && views.length > 0) {
              if (views[0].getAttributes().multiview) {
                // We should have only one view in MV case
                if (!reported) {
                  console.log("Rendering with multiview!");
                  reported = true;
                }
                let view = views[0];
                let viewport = view.getViewport();
                gl.bindFramebuffer(gl.FRAMEBUFFER, view.framebuffer);
                renderFrame(gl, viewport, frameData.leftProjectionMatrix, frameData.leftViewMatrix, viewport, frameData.rightProjectionMatrix, frameData.rightViewMatrix, t, 1);
              }
              else {
                //renderFrame(gl, viewports[0], frameData.leftProjectionMatrix, frameData.leftViewMatrix, viewports[1], frameData.rightProjectionMatrix, frameData.rightViewMatrix, t, 2);
                let VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
                renderFrame(gl, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, t, 1);
              }

              // If we're currently presenting to the VRDisplay we need to
              // explicitly indicate we're done rendering.
              vrDisplay.submitFrame();
            }
            else {
                let VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
                renderFrame(gl, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, VP, frameData.leftProjectionMatrix, frameData.leftViewMatrix, t, 1);

              // If we're currently presenting to the VRDisplay we need to
              // explicitly indicate we're done rendering.
              vrDisplay.submitFrame();
            }
          } else {
            //window.requestAnimationFrame(onAnimationFrame);
            // When not presenting render a mono view that still takes pose into
            // account.
            let VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
            renderFrame(gl, VP, projMatrix, viewMatrix, VP, projMatrix, viewMatrix, t, 1);

            reported = false;
          }
        } else {
          //console.log("vrDisplay = " + vrDisplay);
          window.requestAnimationFrame(onAnimationFrame);

          // No VRDisplay found.
          let VP =  { x: 0, y: 0, width:webglCanvas.width, height:webglCanvas.height };
          renderFrame(gl, VP, projMatrix, viewMatrix, VP, projMatrix, viewMatrix, t, 1);
        }
        simulationAdvance(t);
      }


      window.requestAnimationFrame(onAnimationFrame);
    

      })();
    </script>
  </body>
</html>
